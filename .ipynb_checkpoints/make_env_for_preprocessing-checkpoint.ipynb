{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import datetime as dt\n",
    "import ca_util as ca\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터값을 설정합니다.(conf 파일 적용 예정)\n",
    "output_dir = './output/'\n",
    "data_dir = './data/'\n",
    "seed = 0\n",
    "\n",
    "parameter_id = 'default'\n",
    "datafile = 'dataset_all.csv'\n",
    "resp_success = ['지속(정책)', '지속사용(고객혜택제공)', '지속(혜택)']\n",
    "\n",
    "# 서비스 위해 학습 환경 저장 합니다.\n",
    "preprocess_object = {}       # 학습 환경 전체를 저장합니다.\n",
    "preprocess_del_col = []      # 학습 시 삭제한 컬럼 목록 저장합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset/cam_dataset_all.csv\", delimiter=\",\", encoding='UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resp_func (resp):\n",
    "    if resp in resp_success:\n",
    "        return 'Y'\n",
    "    else:\n",
    "        return 'N'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['resp'] = dataset['반응4'].map(resp_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    # '_ID' 항목 삭제 합니다.\n",
    "    if col.upper().endswith('ID') or col.upper().endswith('_ID.1'):\n",
    "        del dataset[col]\n",
    "        preprocess_del_col.append(col)\n",
    "    # '_DATE' 항목 삭제 합니다.\n",
    "    elif col.upper().endswith('일자') or col.upper().endswith('_DATE') or col.upper().endswith('_DATE.1') or col.upper().endswith('_DT.1'):\n",
    "        del dataset[col]\n",
    "        preprocess_del_col.append(col)\n",
    "    # 범주가 1 이하 항목 삭제 합니다.\n",
    "    elif dataset[col].value_counts().shape[0] <=1:\n",
    "        del dataset[col]\n",
    "        preprocess_del_col.append(col)\n",
    "    # 범주범위가 특정 % 이상 항목 삭제 합니다.\n",
    "    elif dataset[col].value_counts().shape[0]/dataset.shape[0] > 0.50:\n",
    "        del dataset[col]\n",
    "        preprocess_del_col.append(col)\n",
    "    # Others 항목 처리 합니다.\n",
    "    else:\n",
    "    \t# 데이터 중 99999 -> 0 변환 합니다.\n",
    "    \tdataset[col] = dataset[col].replace(99999,0)\n",
    "    \t# 데이터 중 999 -> 0 변환 합니다.\n",
    "    \tdataset[col] = dataset[col].replace(999,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 삭제컬럼 입니다.\n",
    "del_list = ['고객번호','반응1','반응2','반응3','반응4']\n",
    "for del_col in del_list:\n",
    "    try:\n",
    "        del dataset[del_col]\n",
    "        preprocess_del_col.append(del_col)\n",
    "    except Exception as e:\n",
    "        print('Deleting Exception :',e)\n",
    "\n",
    "# 예외 처리 합니다.\n",
    "#dataset['AGE_ITG_CD'] = dataset['AGE_ITG_CD'].astype(str)\n",
    "\n",
    "# 삭제 항목 학습 환경에 저장합니다.\n",
    "preprocess_object['del_col'] = preprocess_del_col\n",
    "\n",
    "# 데이터중 null -> 0 처리 합니다.\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "# 훈련/테스트 데이터 분리 후 전체/훈련/테스트 데이터 저장 합니다.\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=seed)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "OneHot Encoding 처리 합니다.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "전처리 환경 저장 합니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 데이터 저장합니다.\n",
    "dataset.to_csv('./output/preprocess_dataset_all.csv', mode='w', encoding='utf-8', header=1, index=0)\n",
    "train_dataset.to_csv('./output/train_dataset.csv', mode='w', encoding='utf-8', header=1, index=0)\n",
    "test_dataset.to_csv('./output/test_dataset.csv', mode='w', encoding='utf-8', header=1, index=0)\n",
    "\n",
    "# 연속형/이산형 데이터 분리 합니다.\n",
    "number_df = dataset.select_dtypes(include=['int64','float64'])\n",
    "object_df = dataset.select_dtypes(include=['object'])\n",
    "\n",
    "# 연속형 데이터 표준화 합니다.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(number_df)\n",
    "\n",
    "# scaler 학습 환경에 저장 합니다.\n",
    "preprocess_object['scaler'] = scaler\n",
    "\n",
    "# 이산형 데이터 OneHot Encoding 합니다.\n",
    "ca.log_print('OneHot Encoding 처리 합니다.')\n",
    "categorical = list(object_df.columns)\n",
    "for cat in categorical:\n",
    "    oh_encoder = {}\n",
    "\n",
    "    # LabelEncoder 사용 합니다.\n",
    "    le = preprocessing.LabelEncoder()       # LabelEncoder 사용을 위해 선언합니다.\n",
    "    le.fit(dataset[cat].astype(str))        # 범주형 카테고리 정의 합니다.\n",
    "    cat_arr = le.transform(dataset[cat])    # 범주형 카테고리 숫자로 변환 합니다.\n",
    "\n",
    "    # 범주형(숫자변환) OneHot Encoder 사용 합니다.\n",
    "    ohe = preprocessing.OneHotEncoder(categories='auto')     # OneHotEncoder 사용을 위해 선언합니다.\n",
    "    ohe.fit(cat_arr.reshape(-1,1))\n",
    "\n",
    "    # 학습과 예측(서비스) 사용위하 le & ohe Object로 저장합니다.\n",
    "    oh_encoder['LabelEncoder'] = le\n",
    "    oh_encoder['OneHotEncoder'] = ohe\n",
    "\n",
    "    # 항목별 OneHot 정보 저장합니다.\n",
    "    preprocess_object[cat] = oh_encoder\n",
    "\n",
    "    # le & ohe 삭제 합니다\n",
    "    del ohe\n",
    "    del le\n",
    "\n",
    "# 데이터 전처리 환경 파일(pickle)로 저장 합니다.\n",
    "ca.log_print('전처리 환경 저장 합니다.')\n",
    "with open('./output/preprocess_object.pickle', 'wb') as f:\n",
    "    pickle.dump(preprocess_object, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
